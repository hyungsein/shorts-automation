"""
ðŸŽ¨ Image Agent - Generates images using diffusers (local, no server needed)
"""

import asyncio
import random
from pathlib import Path
from typing import Optional

import httpx
import torch
from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler
from PIL import Image

from ..models import ImageResult
from .base import BaseAgent


class ImageAgent(BaseAgent[list[ImageResult]]):
    """Agent for generating images with Stable Diffusion (diffusers)"""

    # Model path - MeinaMix V11 (ê·€ì—¬ìš´ ë¡œë§¨ìŠ¤ ìŠ¤íƒ€ì¼)
    MODEL_PATH = Path.home(
    ) / "ComfyUI" / "models" / "checkpoints" / "meinamix_v11.safetensors"

    # HuggingFace fallback model
    HF_MODEL = "Meina/MeinaMix_V11"

    @property
    def name(self) -> str:
        return "ðŸŽ¨ ImageAgent"

    # ì‡¼ì¸ ìš© ìºë¦­í„° ìŠ¤íƒ€ì¼ - ê¸€ëž˜ë¨¸ ì˜¤í”¼ìŠ¤ ì—¬ìº
    CHARACTER_BASE_PROMPT = """
    masterpiece, best quality, beautiful detailed eyes,
    1girl, office lady, business suit, pencil skirt,
    large breasts, slim waist, attractive body,
    pretty face, makeup, long hair,
    soft lighting, clean background
    """.strip()

    NEGATIVE_PROMPT = """
    ugly, deformed, noisy, blurry, low quality,
    bad anatomy, bad proportions, watermark, text,
    realistic, photo, nsfw, nude, naked, explicit,
    extra fingers, mutated hands, poorly drawn face,
    flat chest, child, loli, underage
    """.strip()

    def __init__(self):
        super().__init__()
        self._pipe: Optional[StableDiffusionPipeline] = None

    def _load_pipeline(self) -> StableDiffusionPipeline:
        """Load the Stable Diffusion pipeline (lazy loading)"""
        if self._pipe is not None:
            return self._pipe

        self.log("Loading MeinaMix model (first time may take a while)...")

        # Check for Apple Silicon MPS
        if torch.backends.mps.is_available():
            device = "mps"
            # MPSì—ì„œëŠ” float32ê°€ ë” ì•ˆì •ì 
            dtype = torch.float32
            self.log("Using Apple Silicon MPS acceleration ðŸŽ")
        elif torch.cuda.is_available():
            device = "cuda"
            dtype = torch.float16
            self.log("Using NVIDIA CUDA acceleration ðŸŸ¢")
        else:
            device = "cpu"
            dtype = torch.float32
            self.log("Using CPU (this will be slow) ðŸ¢")

        # Try local model first, then HuggingFace
        if self.MODEL_PATH.exists():
            self.log(f"Loading local model: {self.MODEL_PATH.name}")
            self._pipe = StableDiffusionPipeline.from_single_file(
                str(self.MODEL_PATH),
                torch_dtype=dtype,
                use_safetensors=True,
            )
        else:
            self.log(
                f"Local model not found, downloading from HuggingFace: {self.HF_MODEL}"
            )
            self._pipe = StableDiffusionPipeline.from_pretrained(
                self.HF_MODEL,
                torch_dtype=dtype,
                use_safetensors=True,
            )

        # Use faster scheduler
        self._pipe.scheduler = DPMSolverMultistepScheduler.from_config(
            self._pipe.scheduler.config)

        self._pipe = self._pipe.to(device)

        # Memory optimization
        self._pipe.enable_attention_slicing()

        self.log("Model loaded successfully! âœ¨")
        return self._pipe

    async def run(
            self,
            prompts: list[str],
            output_dir: Path,
            character_prompt: Optional[str] = None,
            width: int = 512,  # SD 1.5 ê¸°ë³¸ í•´ìƒë„
            height: int = 768,  # ì„¸ë¡œë¡œ ê¸¸ê²Œ (ì‡¼ì¸ ìš©)
    ) -> list[ImageResult]:
        """Generate multiple images for the video"""
        self.log(f"Generating {len(prompts)} images...")

        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        base_prompt = character_prompt or self.CHARACTER_BASE_PROMPT
        results = []

        # Load pipeline once
        pipe = self._load_pipeline()

        for i, scene_prompt in enumerate(prompts):
            # ì¹´ë©”ë¼ íš¨ê³¼ì™€ í”„ë¡¬í”„íŠ¸ ë¶„ë¦¬ (format: "effect|prompt")
            effect = "static"
            actual_prompt = scene_prompt
            if "|" in scene_prompt:
                parts = scene_prompt.split("|", 1)
                effect = parts[0].strip()
                actual_prompt = parts[1].strip()

            # Combine character base + scene-specific prompt
            full_prompt = f"{base_prompt}, {actual_prompt}"

            self.log(f"Generating image {i+1}/{len(prompts)} [{effect}]...")

            image_path = output_dir / f"image_{i:03d}.png"

            try:
                # Run generation in thread pool (sync -> async)
                image = await asyncio.get_event_loop().run_in_executor(
                    None, lambda: self._generate_sync(pipe, full_prompt, width,
                                                      height))

                # Resize to shorts format (9:16) - 1080x1920
                shorts_image = self._resize_for_shorts(image)
                shorts_image.save(image_path)

                # íš¨ê³¼ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ ì•žì— ìœ ì§€ (video_agentì—ì„œ ì‚¬ìš©)
                results.append(
                    ImageResult(
                        file_path=image_path,
                        prompt=f"{effect}|{actual_prompt}",
                        index=i,
                    ))
                self.log(f"âœ“ Image {i+1} saved")
            except Exception as e:
                self.log(f"Failed to generate image {i}: {e}")

        self.log(f"Generated {len(results)} images")
        return results

    def _generate_sync(
        self,
        pipe: StableDiffusionPipeline,
        prompt: str,
        width: int,
        height: int,
    ) -> Image.Image:
        """Synchronous image generation (called in thread pool)"""
        result = pipe(
            prompt=prompt,
            negative_prompt=self.NEGATIVE_PROMPT,
            width=width,
            height=height,
            num_inference_steps=25,
            guidance_scale=7.0,
        )
        return result.images[0]

    def _resize_for_shorts(self, image: Image.Image) -> Image.Image:
        """
        Resize image for YouTube Shorts with safe zone consideration.
        
        YouTube Shorts UI overlay:
        - Top ~15%: ì±„ë„ëª…, íŒ”ë¡œìš° ë²„íŠ¼, ê²€ìƒ‰ ë“±
        - Bottom ~20%: ì¢‹ì•„ìš”, ëŒ“ê¸€, ê³µìœ , ìžë§‰ ì˜ì—­
        
        ì „ëžµ: 1024x1024 ì´ë¯¸ì§€ë¥¼ ì¤‘ì•™ì— ë°°ì¹˜í•˜ê³  ìœ„ì•„ëž˜ì— ë¸”ëŸ¬ ë°°ê²½ ì¶”ê°€
        """
        target_w, target_h = 1080, 1920

        # 1. ì´ë¯¸ì§€ë¥¼ target_wì— ë§žê²Œ ë¦¬ì‚¬ì´ì¦ˆ (ë¹„ìœ¨ ìœ ì§€)
        img_w, img_h = image.size
        scale = target_w / img_w
        new_w = target_w
        new_h = int(img_h * scale)

        resized = image.resize((new_w, new_h), Image.Resampling.LANCZOS)

        # 2. ìƒˆ ìº”ë²„ìŠ¤ ìƒì„± (1080x1920)
        # ë°°ê²½: ì´ë¯¸ì§€ ê°€ìž¥ìžë¦¬ ìƒ‰ìƒ ê¸°ë°˜ ê·¸ë¼ë°ì´ì…˜ íš¨ê³¼
        canvas = Image.new('RGB', (target_w, target_h), (20, 20, 25))

        # 3. ë¸”ëŸ¬ëœ ë°°ê²½ ì´ë¯¸ì§€ ìƒì„± (ìœ„ì•„ëž˜ ì±„ìš°ê¸°ìš©)
        from PIL import ImageFilter

        # ì´ë¯¸ì§€ë¥¼ ì „ì²´ ìº”ë²„ìŠ¤ í¬ê¸°ë¡œ ëŠ˜ë ¤ì„œ ë¸”ëŸ¬ (ë°°ê²½ìš©)
        bg_image = image.resize((target_w, target_h), Image.Resampling.LANCZOS)
        bg_blurred = bg_image.filter(ImageFilter.GaussianBlur(radius=30))

        # ë¸”ëŸ¬ ë°°ê²½ì„ ì–´ë‘¡ê²Œ (ìžë§‰ ê°€ë…ì„±)
        from PIL import ImageEnhance
        enhancer = ImageEnhance.Brightness(bg_blurred)
        bg_darkened = enhancer.enhance(0.4)  # 40% ë°ê¸°

        canvas.paste(bg_darkened, (0, 0))

        # 4. ë©”ì¸ ì´ë¯¸ì§€ë¥¼ ì¤‘ì•™ë³´ë‹¤ ì•½ê°„ ìœ„ì— ë°°ì¹˜ (í•˜ë‹¨ ìžë§‰ ê³µê°„ í™•ë³´)
        # ìƒë‹¨ 15%, í•˜ë‹¨ 20% = safe zone ë°–
        # ì´ë¯¸ì§€ë¥¼ ì•½ê°„ ìœ„ë¡œ ì˜¬ë ¤ì„œ í•˜ë‹¨ì— ìžë§‰ ê³µê°„ í™•ë³´

        top_margin = int(target_h * 0.12)  # ìƒë‹¨ 12% ì—¬ë°±
        bottom_margin = int(target_h * 0.22)  # í•˜ë‹¨ 22% ì—¬ë°± (ìžë§‰ + UI)

        available_height = target_h - top_margin - bottom_margin

        if new_h > available_height:
            # ì´ë¯¸ì§€ê°€ safe zoneë³´ë‹¤ í¬ë©´ ì¶•ì†Œ
            scale = available_height / new_h
            new_w = int(new_w * scale)
            new_h = int(new_h * scale)
            resized = image.resize((new_w, new_h), Image.Resampling.LANCZOS)

        # ì¤‘ì•™ ì •ë ¬ (ìˆ˜í‰), safe zone ë‚´ ì¤‘ì•™ (ìˆ˜ì§)
        x = (target_w - new_w) // 2
        y = top_margin + (available_height - new_h) // 2

        canvas.paste(resized, (x, y))

        return canvas

    async def generate_character_sheet(
        self,
        character_description: str,
        output_dir: Path,
        num_variations: int = 5,
    ) -> list[ImageResult]:
        """Generate multiple variations of the same character"""

        expressions = [
            "happy expression, smiling",
            "surprised expression, shocked",
            "thinking expression, curious",
            "sad expression, melancholy",
            "excited expression, energetic",
        ]

        prompts = [
            f"{character_description}, {expr}"
            for expr in expressions[:num_variations]
        ]

        return await self.run(prompts, output_dir)

    async def search_and_download_image(
        self,
        query: str,
        output_path: Path,
    ) -> Optional[Path]:
        """
        Unsplashì—ì„œ ë¬´ë£Œ ì´ë¯¸ì§€ ê²€ìƒ‰ & ë‹¤ìš´ë¡œë“œ
        ì£¼ì œì— ë§žëŠ” ì‹¤ì œ ì´ë¯¸ì§€ (ì€ìˆ˜ì €, ì¹´íŽ˜, í—¬ìŠ¤ìž¥ ë“±)
        """
        self.log(f"Searching image for: {query}")

        try:
            # Unsplash API (ë¬´ë£Œ, API í‚¤ ë¶ˆí•„ìš”í•œ ë°©ì‹)
            async with httpx.AsyncClient(timeout=30) as client:
                # ê²€ìƒ‰ URL (source.unsplash.com ë¦¬ë‹¤ì´ë ‰íŠ¸ ì‚¬ìš©)
                search_url = f"https://source.unsplash.com/800x600/?{query}"

                response = await client.get(search_url, follow_redirects=True)

                if response.status_code == 200:
                    # ì´ë¯¸ì§€ ì €ìž¥
                    output_path.parent.mkdir(parents=True, exist_ok=True)

                    with open(output_path, "wb") as f:
                        f.write(response.content)

                    # ì‡¼ì¸  í¬ë§·ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
                    img = Image.open(output_path)
                    resized = self._resize_for_shorts(img)
                    resized.save(output_path)

                    self.log(f"âœ“ Downloaded: {query}")
                    return output_path

        except Exception as e:
            self.log(f"Failed to search image: {e}")

        return None

    async def get_topic_image(
        self,
        topic: str,
        output_dir: Path,
    ) -> Optional[ImageResult]:
        """
        ì£¼ì œì— ë§žëŠ” ëŒ€í‘œ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
        ì˜ˆ: "ì€ìˆ˜ì €" â†’ ì€ìˆ˜ì € ì´ë¯¸ì§€, "ì¹´íŽ˜" â†’ ì¹´íŽ˜ ì´ë¯¸ì§€
        """
        # í•œêµ­ì–´ â†’ ì˜ì–´ í‚¤ì›Œë“œ ë§¤í•‘ (ê²€ìƒ‰ìš©)
        keyword_map = {
            "ì€ìˆ˜ì €": "silver spoon wealth",
            "ê¸ˆìˆ˜ì €": "gold spoon luxury",
            "í™ìˆ˜ì €": "poor struggle",
            "ì¹´íŽ˜": "coffee shop barista",
            "í—¬ìŠ¤ìž¥": "gym fitness",
            "íšŒì‚¬": "office workplace",
            "ì§ìž¥": "corporate office",
            "ì•Œë°”": "part time job",
            "ì—°ì• ": "couple love",
            "ì¸": "romantic dating",
            "ì¹œêµ¬": "friendship friends",
            "ê°€ì¡±": "family",
            "í•™êµ": "school student",
            "ëŒ€í•™": "university college",
            "ë©´ì ‘": "job interview",
            "ì´ì§": "career change",
            "í‡´ì‚¬": "quit job resignation",
            "ì›”ê¸‰": "salary paycheck money",
            "ë¶€ìž": "rich wealthy luxury",
            "ì—¬í–‰": "travel vacation",
        }

        # ì£¼ì œì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        search_query = None
        for korean, english in keyword_map.items():
            if korean in topic:
                search_query = english
                break

        if not search_query:
            # ë§¤í•‘ ì—†ìœ¼ë©´ ì£¼ì œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            search_query = topic

        output_path = output_dir / "topic_image.png"
        result = await self.search_and_download_image(search_query,
                                                      output_path)

        if result:
            return ImageResult(
                file_path=result,
                prompt=f"searched: {search_query}",
                index=-1,  # íŠ¹ë³„ ì´ë¯¸ì§€ í‘œì‹œ
            )

        return None

    async def check_connection(self) -> bool:
        """Check if model is available (local file or can download)"""
        if self.MODEL_PATH.exists():
            return True
        # Can always download from HuggingFace
        return True
