"""
ğŸ¬ Video Agent - Creates final short video with images, TTS, and subtitles
"""

import asyncio
from pathlib import Path

from moviepy import (
    AudioFileClip,
    ColorClip,
    CompositeVideoClip,
    ImageClip,
    TextClip,
    concatenate_videoclips,
)
from PIL import Image

from ..config import settings
from ..models import AudioResult, ImageResult, Script, VideoResult
from .base import BaseAgent


class VideoAgent(BaseAgent[VideoResult]):
    """Agent for creating short videos with images and subtitles"""

    @property
    def name(self) -> str:
        return "ğŸ¬ VideoAgent"

    # Short video dimensions (9:16 aspect ratio)
    WIDTH = 1080
    HEIGHT = 1920

    async def run(
        self,
        images: list[ImageResult],
        audio: AudioResult,
        script: Script,
        output_path: Path,
    ) -> VideoResult:
        """Create final short video with images and subtitles"""
        self.log("Creating video...")

        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # Load audio
        audio_clip = AudioFileClip(str(audio.file_path))
        duration = audio_clip.duration

        # Create image slideshow
        if images:
            bg_clip = self._create_image_slideshow(images, duration)
        else:
            bg_clip = self._create_gradient_background(duration)

        # Generate subtitles
        subtitle_clips = await self._generate_subtitles(script, duration)

        # Compose final video
        final_clip = CompositeVideoClip([bg_clip] + subtitle_clips,
                                        size=(self.WIDTH, self.HEIGHT))
        final_clip = final_clip.with_audio(audio_clip)

        # Export
        self.log(f"Exporting video to {output_path}...")
        final_clip.write_videofile(
            str(output_path),
            fps=30,
            codec="libx264",
            audio_codec="aac",
            preset="medium",
            threads=4,
        )

        # Cleanup
        audio_clip.close()
        bg_clip.close()
        final_clip.close()

        self.log(f"Video created: {output_path}")

        return VideoResult(
            file_path=output_path,
            duration=duration,
            resolution=(self.WIDTH, self.HEIGHT),
        )

    def _create_image_slideshow(
        self,
        images: list[ImageResult],
        duration: float,
    ) -> CompositeVideoClip:
        """Create dynamic slideshow with intentional camera effects"""
        if not images:
            return self._create_gradient_background(duration)

        time_per_image = duration / len(images)
        image_clips = []

        for i, img_result in enumerate(images):
            img_path = img_result.file_path

            # Load image
            img_clip = ImageClip(str(img_path))
            img_clip = self._resize_to_fit(img_clip)

            # í”„ë¡¬í”„íŠ¸ì—ì„œ ì¹´ë©”ë¼ íš¨ê³¼ ì¶”ì¶œ (format: "effect|prompt")
            effect_type = "static"
            if "|" in img_result.prompt:
                parts = img_result.prompt.split("|", 1)
                effect_type = parts[0].strip()

            # ìœ íš¨í•œ íš¨ê³¼ì¸ì§€ í™•ì¸
            valid_effects = [
                "zoom_in", "zoom_out", "pan_left", "pan_right", "static",
                "zoom_pulse"
            ]
            if effect_type not in valid_effects:
                effect_type = "static"

            # ì¤Œ/íŒ¬ íš¨ê³¼ ì ìš©
            img_clip = self._apply_dynamic_effect(img_clip, effect_type,
                                                  time_per_image)

            # Set timing
            img_clip = img_clip.with_start(i * time_per_image)
            img_clip = img_clip.with_duration(time_per_image)

            image_clips.append(img_clip)

        # Create background
        bg = ColorClip(
            size=(self.WIDTH, self.HEIGHT),
            color=(15, 15, 20),
            duration=duration,
        )

        return CompositeVideoClip([bg] + image_clips,
                                  size=(self.WIDTH, self.HEIGHT))

    def _apply_dynamic_effect(
        self,
        clip: ImageClip,
        effect_type: str,
        duration: float,
    ) -> ImageClip:
        """
        ë‹¤ì´ë‚˜ë¯¹ ì¤Œ/íŒ¬ íš¨ê³¼ ì ìš©
        - zoom_in: ì²œì²œíˆ ì¤Œì¸
        - zoom_out: ì¤Œì•„ì›ƒ
        - pan_left/right: ì¢Œìš° íŒ¨ë‹
        - zoom_pulse: ì¤Œì¸í–ˆë‹¤ ì¤Œì•„ì›ƒ
        """

        # ì¤Œ ë²”ìœ„ (1.0 = ì›ë³¸, 1.2 = 20% í™•ëŒ€)
        zoom_start = 1.0
        zoom_end = 1.15

        def zoom_in_effect(t):
            """ì‹œê°„ì— ë”°ë¼ ì¤Œì¸"""
            progress = t / duration if duration > 0 else 0
            scale = zoom_start + (zoom_end - zoom_start) * progress
            return scale

        def zoom_out_effect(t):
            """ì‹œê°„ì— ë”°ë¼ ì¤Œì•„ì›ƒ"""
            progress = t / duration if duration > 0 else 0
            scale = zoom_end - (zoom_end - zoom_start) * progress
            return scale

        def zoom_pulse_effect(t):
            """ì¤Œì¸í–ˆë‹¤ê°€ ì¤Œì•„ì›ƒ"""
            import math
            progress = t / duration if duration > 0 else 0
            # ì‚¬ì¸ ê³¡ì„ ìœ¼ë¡œ ë¶€ë“œëŸ½ê²Œ
            scale = zoom_start + (zoom_end - zoom_start) * math.sin(
                progress * math.pi)
            return scale

        if effect_type == "zoom_in":
            return clip.resized(lambda t: zoom_in_effect(t))
        elif effect_type == "zoom_out":
            return clip.resized(lambda t: zoom_out_effect(t))
        elif effect_type == "zoom_pulse":
            return clip.resized(lambda t: zoom_pulse_effect(t))
        elif effect_type == "pan_left":
            # ì˜¤ë¥¸ìª½ì—ì„œ ì™¼ìª½ìœ¼ë¡œ ì´ë™
            def pan_left_pos(t):
                progress = t / duration if duration > 0 else 0
                x = int(50 - 100 * progress)  # 50 -> -50
                return (x, 'center')

            return clip.with_position(pan_left_pos)
        elif effect_type == "pan_right":
            # ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™
            def pan_right_pos(t):
                progress = t / duration if duration > 0 else 0
                x = int(-50 + 100 * progress)  # -50 -> 50
                return (x, 'center')

            return clip.with_position(pan_right_pos)
        else:
            return clip

    def _resize_to_fit(self, clip: ImageClip) -> ImageClip:
        """Resize image clip to fit 9:16 with extra margin for zoom effects"""
        # Calculate scale to fill the frame (ì¤Œ íš¨ê³¼ë¥¼ ìœ„í•´ 20% ë” í¬ê²Œ)
        scale_w = self.WIDTH / clip.w
        scale_h = self.HEIGHT / clip.h
        scale = max(scale_w, scale_h) * 1.2  # 20% ì—¬ìœ 

        new_w = int(clip.w * scale)
        new_h = int(clip.h * scale)

        clip = clip.resized((new_w, new_h))

        # Center the clip
        x_pos = (self.WIDTH - new_w) // 2
        y_pos = (self.HEIGHT - new_h) // 2

        return clip.with_position((x_pos, y_pos))

    async def _generate_subtitles(
        self,
        script: Script,
        duration: float,
    ) -> list[TextClip]:
        """
        Generate animated subtitle clips - ìš”ì¦˜ ì‡¼ì¸  ìŠ¤íƒ€ì¼
        
        íŠ¹ì§•:
        - ì§§ê²Œ ì§§ê²Œ (2-4 ë‹¨ì–´ì”©)
        - ë¹ ë¥´ê²Œ ì „í™˜ (ë‹µë‹µí•˜ì§€ ì•Šê²Œ)
        - í•˜ë‹¨ safe zoneì— ë°°ì¹˜
        - í° ê¸€ì”¨ + í…Œë‘ë¦¬ (ê°€ë…ì„±)
        """
        subtitle_clips = []

        # ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¨¼ì € ë¶„ë¦¬
        text = script.full_text

        # ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œë¡œ ë¬¸ì¥ ë¶„ë¦¬
        import re
        sentences = re.split(r'(?<=[.?!])\s+', text)

        # ê° ë¬¸ì¥ì„ ì§§ì€ êµ¬ì ˆë¡œ ë¶„ë¦¬ (2-4 ë‹¨ì–´)
        phrases = []
        for sentence in sentences:
            words = sentence.split()

            # í•œêµ­ì–´ íŠ¹ì„±ìƒ 2-3 ë‹¨ì–´ê°€ ì ë‹¹ (ê¸´ ë‹¨ì–´ ë§ìŒ)
            chunk_size = 2 if any(len(w) > 5 for w in words) else 3

            for i in range(0, len(words), chunk_size):
                chunk = words[i:i + chunk_size]
                if chunk:
                    phrase = " ".join(chunk)
                    # ë„ˆë¬´ ì§§ì€ êµ¬ì ˆì€ ë‹¤ìŒê³¼ í•©ì¹˜ê¸°
                    if phrases and len(phrase) < 4 and len(phrases[-1]) < 15:
                        phrases[-1] += " " + phrase
                    else:
                        phrases.append(phrase)

        if not phrases:
            return subtitle_clips

        # ê° êµ¬ì ˆì˜ í‘œì‹œ ì‹œê°„ ê³„ì‚°
        # ìµœì†Œ 0.4ì´ˆ, ìµœëŒ€ 1.5ì´ˆ (ê¸€ììˆ˜ì— ë¹„ë¡€)
        total_chars = sum(len(p) for p in phrases)

        # ì‹œê°„ ë°°ë¶„
        phrase_times = []
        current_time = 0.0

        for phrase in phrases:
            # ê¸€ììˆ˜ ê¸°ë°˜ ì‹œê°„ (í•œ ê¸€ìë‹¹ ì•½ 0.08ì´ˆ, ìµœì†Œ 0.5ì´ˆ)
            char_time = len(phrase) * 0.08
            phrase_duration = max(0.5, min(1.5, char_time))

            phrase_times.append({
                "text": phrase,
                "start": current_time,
                "duration": phrase_duration
            })
            current_time += phrase_duration

        # ì „ì²´ ì‹œê°„ì— ë§ê²Œ ìŠ¤ì¼€ì¼ ì¡°ì •
        if current_time > 0:
            scale = duration / current_time
            for pt in phrase_times:
                pt["start"] *= scale
                pt["duration"] *= scale

        self.log(f"Creating {len(phrases)} subtitle segments")

        for pt in phrase_times:
            # ìë§‰ í´ë¦½ ìƒì„±
            txt_clip = TextClip(
                text=pt["text"],
                font_size=72,  # ë” í° ê¸€ì”¨
                color="white",
                stroke_color="black",
                stroke_width=4,  # ë” ë‘êº¼ìš´ í…Œë‘ë¦¬
                font="Arial-Bold",
                method="caption",
                size=(self.WIDTH - 120, None),
                text_align="center",
            )

            # í•˜ë‹¨ safe zoneì— ë°°ì¹˜ (UI í”¼í•´ì„œ)
            # í™”ë©´ì˜ ì•½ 70% ìœ„ì¹˜ (í•˜ë‹¨ 30% ì˜ì—­ ì¤‘ ìœ„ìª½)
            txt_clip = txt_clip.with_position(("center", self.HEIGHT * 0.68))
            txt_clip = txt_clip.with_start(pt["start"])
            txt_clip = txt_clip.with_duration(pt["duration"])

            subtitle_clips.append(txt_clip)

        return subtitle_clips

    def _create_gradient_background(self, duration: float) -> ColorClip:
        """Create a simple dark background"""
        return ColorClip(
            size=(self.WIDTH, self.HEIGHT),
            color=(15, 15, 20),
            duration=duration,
        )
